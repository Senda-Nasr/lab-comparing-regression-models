{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62e194a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Import the necessary libraries.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82012160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load the `customer_value_analysis.csv` into the variable `customer_df`.\n",
    "customer_df = pd.read_csv('files_for_lab\\customer_value_analysis.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8563ea32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. First look at its main features (`head`, `shape`, `info`).\n",
    "display(customer_df.head())\n",
    "display(customer_df.shape)\n",
    "display(customer_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510841cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Rename the columns so they follow the _PE8_ (snake case: lowecase_with_underscores).\n",
    "def col_rename (dframe: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function renames column names by removing spaces and converting to lower case\n",
    "    Inputs: dframe of type pandas dataframe\n",
    "    Outputs: returns the dataframe with the renamed columns\n",
    "    \"\"\"\n",
    "    cols =[]\n",
    "    for x in dframe.columns:\n",
    "        if isinstance(x, str):\n",
    "            cols.append(x.lower().replace(' ', '_'))\n",
    "        else:\n",
    "            cols.append(x)\n",
    "    if 'st' in cols:\n",
    "        index = cols.index('st')   \n",
    "        cols[index]='state'\n",
    "\n",
    "    dframe.columns=cols\n",
    "    return dframe\n",
    "\n",
    "col_rename(customer_df)\n",
    "customer_df.rename(columns={'employmentstatus': 'employment_status'}, inplace=True)\n",
    "\n",
    "customer_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcde540c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Change the type of `effective_to_date` column to DateTime format.\n",
    "customer_df['effective_to_date'] = pd.to_datetime(customer_df['effective_to_date'])\n",
    "customer_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5ead1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Check `NaN` values per column.\n",
    "customer_df.isna().sum() #there are no NaN values in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ff5eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Define a function that given an input dataframe, returns two dataframes: one with numerical columns and another with categorical columns of the input dataframe.\n",
    "def split_df(df: pd.DataFrame):\n",
    "    num_df = df.select_dtypes(include='number')\n",
    "    cat_df = df.select_dtypes(include='object')\n",
    "    \n",
    "    return num_df, cat_df\n",
    "\n",
    "#calling the function\n",
    "numerical_df, categorical_df = split_df(customer_df)\n",
    "display(numerical_df)\n",
    "display(categorical_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdb6658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Drop any ID column.\n",
    "categorical_df.drop('customer', axis=1, inplace=True)\n",
    "display(categorical_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78adf287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Get the correlation matrix for the numerical variables. What is the pair of numerical variables that have the highest correlation? It makes sense, why?\n",
    "correlation_matrix = numerical_df.corr()\n",
    "display(correlation_matrix)\n",
    "\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title(\"Correlation Matrix Heatmap\")\n",
    "plt.show()\n",
    "\n",
    "# The variables with the highest correlation are the monthly_premium_auto and the total_claim_amount\n",
    "# This makes sense because the higher the amount a customer pays for their auto insurance, the more likely the customer is to apply for claims "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d848ab46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Define a function that takes a pandas DataFrame as an input and returns two pandas DataFrames: the first containing numerical continuous columns and the second containing numerical discrete columns of the input dataframe. To this end, it might be helpful to count the number of unique values. The function **must have an optional argument set by default to 36** to discriminate between continuous and discrete columns. Then, use it to create two new dataframes: continuous_df and discrete_df. \n",
    "def split_continuous_discrete(df: pd.DataFrame, threshhold=36):\n",
    "    continuous_cols=[]\n",
    "    discrete_cols=[]\n",
    "    for col in df.columns:\n",
    "        print(col, df[col].nunique())\n",
    "        if df[col].nunique()>threshhold:\n",
    "            continuous_cols.append(col)\n",
    "        else:\n",
    "            discrete_cols.append(col)\n",
    "    \n",
    "    cont_df = df[continuous_cols]\n",
    "    disc_df = df[discrete_cols]\n",
    "    \n",
    "    return cont_df, disc_df\n",
    "\n",
    "continuous_df, discrete_df= split_continuous_discrete(numerical_df)\n",
    "display(continuous_df)\n",
    "display(discrete_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754e5a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Create a function to create a barplot for all the columns of the discrete_df using seaborn, and set the figuresize = (16,16). \n",
    "def plot_bar(df: pd.DataFrame, figsize=(16, 16)):\n",
    "    for col in df.columns:\n",
    "        plt.figure(figsize=figsize)\n",
    "        sns.countplot(x=col, data=df)\n",
    "        plt.xlabel(col)\n",
    "        plt.ylabel('Count')\n",
    "        plt.title(f'Bar Plot for {col}')\n",
    "        plt.show()\n",
    "        \n",
    "plot_bar(discrete_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7573a98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. Create a function to create a histogram for all the columns of the continuous_df using seaborn, and set the figuresize = (16,16)\n",
    "def plot_hist(df: pd.DataFrame, figsize=(16, 16)):\n",
    "    for col in df.columns:\n",
    "        plt.figure(figsize=figsize)\n",
    "        sns.histplot(df[col], bins='auto')\n",
    "        plt.xlabel(col)\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title(f'Histogram for {col}')\n",
    "        plt.show()\n",
    "        \n",
    "plot_hist(continuous_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbe98b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13. According to the previous histogram plots, do you think that you will have to apply any transformation?\n",
    "# Yes, as the histograms are skewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4821d7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 14. Look for outliers in the continuous variables that you have found. Hint: There was a good plot to do that. Define a function to create this kind of plot for the continuous_df.\n",
    "def plot_box(df: pd.DataFrame, figsize=(8,4)):\n",
    "    for col in df.columns:\n",
    "        plt.figure(figsize=figsize)\n",
    "        sns.boxplot(x=df[col])\n",
    "        plt.xlabel(col)\n",
    "        plt.ylabel('Values')\n",
    "        plt.title(f'Box Plot for Outliers of {col}')\n",
    "        plt.show()\n",
    "        \n",
    "plot_box(continuous_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c48417",
   "metadata": {},
   "source": [
    "# Lab Cleaning Categorical Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bd54dc",
   "metadata": {},
   "source": [
    "1) Define a function that given a pandas DataFrame as input creates a **seaborn countplot** of each categorical column. Make sure to sort the bars by frequency ie: the most frequent values should be placed first. Hint: use .value_counts(). In addition, if the amount of unique values of a categorical column (cardinality) is six or more, the corresponding countplot should have the bars placed on the y-axis instead of the x-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74faf2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_countplot (df: pd.DataFrame, figsize=(8,4)):\n",
    "    #df = df.select_dtypes(include='object') #Only if we provide the original dataframe and not the categorical one\n",
    "    #df.drop('customer', axis=1, inplace=True)# Only if we provide the original dataframe and not the categorical one\n",
    "\n",
    "    for col in df.columns:\n",
    "        plt.figure(figsize=figsize)\n",
    "        value_counts = df[col].value_counts()\n",
    "        sorted_values = value_counts.index\n",
    "        if df[col].nunique()>=6:\n",
    "            sns.countplot(y=col, data=df, order=sorted_values)\n",
    "        else:\n",
    "            sns.countplot(x=col, data=df, order=sorted_values)\n",
    "        plt.xlabel(col)\n",
    "        plt.ylabel('Count')\n",
    "        plt.title(f'Count Plot for {col}')\n",
    "        plt.show()\n",
    "                     \n",
    "create_countplot(categorical_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00933f8",
   "metadata": {},
   "source": [
    "2. `policy_type` and `policy` columns are redundant, and what's worse `policy` column has a lot of possible unique values (high cardinality) which will be problematic when they will be dummified with an OneHotEncoder because we will increase a lot the number of columns in the dataframe. Drop the column `policy_type` and transform the column `policy` to three possible values: L1, L2, and L3 using a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac28f427",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_df.drop('policy_type', axis=1, inplace=True)\n",
    "\n",
    "def clean_policy(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df['policy'] = df['policy'].replace({r'.*L1.*': 'L1', r'.*L2.*': 'L2', r'.*L3.*': 'L3'}, regex=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53461cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_df=clean_policy(categorical_df)\n",
    "display(categorical_df.head())\n",
    "categorical_df['policy'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216254b8",
   "metadata": {},
   "source": [
    "3. Time dependency analysis. Use a seaborn line plot using the column `effective_to_date` to see if `total_claim_amount` is bigger at some specific dates. Use a figsize=(10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09f5af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df = customer_df.sort_values(by='effective_to_date')\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.lineplot(x='effective_to_date', y='total_claim_amount', data=customer_df)\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Time Dependency Analysis - Total Claim Amount against Effective to Date')\n",
    "plt.xlabel('Effective To Date')\n",
    "plt.ylabel('Total Claim Amount')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84e252b",
   "metadata": {},
   "source": [
    "4. To continue the analysis define an empty pandas DataFrame, and add the following new columns:\n",
    "* `day` with the day number of `effective_to_date`\n",
    "* `day_name` with the day NAME of `effective_to_date`\n",
    "* `week` with the week of `effective_to_date`\n",
    "* `month` with the month NAME of `effective_to_date`\n",
    "* `total_claim_amount` with `total_claim_amount`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0783e712",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df= pd.DataFrame()\n",
    "new_df['day']= customer_df['effective_to_date'].dt.day\n",
    "new_df['day_name']= customer_df['effective_to_date'].dt.day_name()\n",
    "new_df['week']= customer_df['effective_to_date'].dt.isocalendar().week\n",
    "new_df['month']= customer_df['effective_to_date'].dt.month_name()\n",
    "new_df['total_claim_amount'] = customer_df['total_claim_amount'].copy()\n",
    "display(new_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd0d729",
   "metadata": {},
   "source": [
    "5. Compute the total `target` column aggregated `day_name` rounded to two decimals and then reorder the index of the resulting pandas series using `.reindex(index=list_of_correct_days)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acb9b09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "target_by_day = pd.pivot_table(new_df, values='total_claim_amount', index='day_name', aggfunc='sum').round(2)\n",
    "list_of_correct_days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "target_by_day= target_by_day.reindex(index=list_of_correct_days)\n",
    "target_by_day.columns=['target']\n",
    "target_by_day= target_by_day.reset_index()\n",
    "display(target_by_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298b9603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Use a seaborn line plot to plot the previous series. Do you see some differences by day of the week?\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.lineplot(x='day_name', y='target', data=target_by_day)\n",
    "plt.title('Total Claim Amount  by Day Name')\n",
    "plt.xlabel('Day of the Week')\n",
    "plt.ylabel('Target')\n",
    "plt.show()\n",
    "# I can see that most claims are made on Monday, the first day of the week\n",
    "# This is followed by the weekend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e970e519",
   "metadata": {},
   "source": [
    "7. Get the total number of claims by day of the week name and then reorder the index of the resulting pandas series using `.reindex(index=list_of_correct_values)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527a349e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seems to me the same requirement as the previous question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbbb288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Get the median \"target\" by day of the week name and then sort the resulting values in descending order using .sort_values()\n",
    "median_by_day = pd.pivot_table(new_df, values='total_claim_amount', index='day_name', aggfunc='median').round(2)\n",
    "median_by_day= median_by_day.sort_values(by= 'total_claim_amount', ascending= False)\n",
    "median_by_day.columns=['median_target']\n",
    "median_by_day= median_by_day.reset_index()\n",
    "display(median_by_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b07aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Plot the median \"target\" by day of the week name using a seaborn barplot\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.barplot(x='day_name', y='median_target', data=median_by_day, palette='viridis')\n",
    "plt.xlabel('Day of the Week')\n",
    "plt.ylabel('Median Target')\n",
    "plt.title('Median Target by day of the Week')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabd8041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. What can you conclude from this analysis?\n",
    "# The median value is simimlar for all the days of the week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e8faf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Compute the total `target` column aggregated `month` rounded to two decimals and then reorder the index of the resulting pandas series using .reindex(index=list_of_correct_values)\n",
    "new_df['month'].value_counts(dropna = False) #we only have January and February\n",
    "target_by_month = pd.pivot_table(new_df, values='total_claim_amount', index='month', aggfunc='sum').round(2)\n",
    "list_of_correct_months = ['January', 'February']\n",
    "target_by_month= target_by_month.reindex(index=list_of_correct_months)\n",
    "target_by_month.columns=['target']\n",
    "target_by_month= target_by_month.reset_index()\n",
    "display(target_by_month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642abcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. Can you do a monthly analysis given the output of the previous series? Why?\n",
    "# No,as the output is only showing the total target, and does not give any insights to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45191813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13. Define a function to remove the outliers of a numerical continuous column depending if a value is bigger or smaller than a given amount of standard deviations of the mean (thr=3)\n",
    "def remove_outliers (df: pd.DataFrame, thr=3) ->pd.DataFrame:\n",
    "    for col in df.columns:\n",
    "        std_dev= df[col].std()\n",
    "        mean_value = df[col].mean()\n",
    "        filter_condition = (df[col] >= mean_value - thr * std_dev) & (df[col] <= mean_value + thr * std_dev)\n",
    "        df = df[filter_condition]\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30f7269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14. Use the previous function to remove the outliers of continuous data and to generate a continuous_clean_df.\n",
    "continuous_clean_df=continuous_df.copy()\n",
    "continuous_clean_df=remove_outliers(continuous_clean_df)\n",
    "display(continuous_clean_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f1c226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15. Concatenate the `continuous_cleaned_df`, `discrete_df`, `categorical_df`, and the relevant column of `time_df`. After removing outliers the continuous_cleaned dataframe will have fewer rows (when you concat the individual dataframes using `pd.concat()`) the resulting dataframe will have NaN's because of the different sizes of each dataframe. Use `pd.dropna()` and `.reset_index()` to fix the final dataframe.\n",
    "time_df= new_df.drop(['day_name', 'week', 'total_claim_amount'], axis=1)\n",
    "concatenated_df = pd.concat([categorical_df, discrete_df, continuous_clean_df, time_df], axis=1)\n",
    "#display(concatenated_df)\n",
    "#print(concatenated_df.isna().sum())\n",
    "concatenated_df.dropna(inplace=True)\n",
    "#display(concatenated_df)\n",
    "print(concatenated_df.isna().sum())\n",
    "concatenated_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891626d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16. Reorder the columns of the dataframe to place 'total_claim_amount' as the last column.\n",
    "columns_order = [col for col in concatenated_df.columns if col != 'total_claim_amount']\n",
    "new_column_order = columns_order + ['total_claim_amount']\n",
    "concatenated_df = concatenated_df[new_column_order]\n",
    "display(concatenated_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f91e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 17. Turn the `response` column values into (Yes=1/No=0).\n",
    "concatenated_df['response'] = concatenated_df['response'].replace({'Yes':1, 'No':0})\n",
    "display(concatenated_df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4564b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#18. Reduce the class imbalance in `education` by grouping together [\"Master\",\"Doctor\"] into \"Graduate\" while keeping the other possible values as they are. In this way, you will reduce a bit the class imbalance at the price of losing a level of detail.\n",
    "concatenated_df['education'].replace({'Master': 'Graduate', 'Doctor': 'Graduate'}, inplace=True)\n",
    "display(concatenated_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbed3a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 19. Reduce the class imbalance of the `employmentstatus` column grouping together [\"Medical Leave\", \"Disabled\", \"Retired\"] into \"Inactive\" while keeping the other possible values as they are. In this way, you will reduce a bit the class imbalance at the price of losing a level of detail.\n",
    "values_to_replace = ['Medical Leave', 'Disabled' , 'Retired']\n",
    "concatenated_df['employment_status'] = concatenated_df['employment_status'].replace(values_to_replace, 'Inactive')\n",
    "display(concatenated_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b2e4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20. Deal with column `Gender` turning the values into (1/0).\n",
    "concatenated_df['gender'] = concatenated_df['gender'].replace({'F':1, 'M':0})\n",
    "display(concatenated_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f405495c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 21. Now, deal with `vehicle_class` grouping together \"Sports Car\", \"Luxury SUV\", and \"Luxury Car\" into a common group called `Luxury` leaving the other values as they are. In this way, you will reduce a bit the class imbalance at the price of losing a level of detail.\n",
    "values_to_replace = ['Sports Car', 'Luxury SUV', 'Luxury Car']\n",
    "concatenated_df['vehicle_class'] = concatenated_df['vehicle_class'].replace(values_to_replace, 'Luxury')\n",
    "display(concatenated_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc026c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 22. Now it's time to deal with the **categorical ordinal columns**, assigning a numerical value to each unique value respecting the ìmplicit ordering`. Encode the coverage: \"Premium\" > \"Extended\" > \"Basic\".\n",
    "numerical_values = {\"Basic\": 1, \"Extended\": 2, \"Premium\": 3}\n",
    "concatenated_df['coverage'] = concatenated_df['coverage'].replace(numerical_values)\n",
    "display(concatenated_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a4720f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 23. Encode the column `employmentstatus` as: \"Employed\" > \"Inactive\" > \"Unemployed\".\n",
    "numerical_values = {\"Unemployed\": 1, \"Inactive\": 2, \"Employed\": 3}\n",
    "concatenated_df['employment_status'] = concatenated_df['employment_status'].replace(numerical_values)\n",
    "display(concatenated_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e35a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 24. Encode the column `location_code` as: \"Urban\" > \"Suburban\" > \"Rural\".\n",
    "numerical_values = {\"Rural\": 1, \"Suburban\": 2, \"Urban\": 3}\n",
    "concatenated_df['location_code'] = concatenated_df['location_code'].replace(numerical_values)\n",
    "display(concatenated_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0931fe77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 25. Encode the column `vehicle_size` as: \"Large\" > \"Medsize\" > \"Small\".\n",
    "numerical_values = {\"Small\": 1, \"Medsize\": 2, \"Large\": 3}\n",
    "concatenated_df['vehicle_size'] = concatenated_df['vehicle_size'].replace(numerical_values)\n",
    "display(concatenated_df['vehicle_size'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238ecc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 26. Get a dataframe with the **categorical nominal columns**\n",
    "categorical_nominal_df = concatenated_df.select_dtypes(include='object')\n",
    "categorical_nominal_df.drop('education' , axis=1, inplace=True) #I will drop the eduaction column as it is nominal and not ordinal\n",
    "display(categorical_nominal_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdaf74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#27. Create a list of named `levels` which that has as many elements as categorical nominal columns. Each element must be another list with all the possible unique values of the corresponding categorical nominal column\n",
    "levels= []\n",
    "for col in categorical_nominal_df:\n",
    "    unique_values = categorical_nominal_df[col].unique().tolist()\n",
    "    levels.append(unique_values)\n",
    "display(levels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7b5275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 28. Instantiate an [sklearn OneHotEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html#sklearn.preprocessing.OneHotEncoder) with drop set to `first` and categories to `levels`\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder(drop='first')\n",
    "encoder.fit(categorical_nominal_df)\n",
    "categorical_nom_array= encoder.transform(categorical_nominal_df).toarray()\n",
    "categorical_nominal_encoded = pd.DataFrame(categorical_nom_array, columns=encoder.get_feature_names_out(), \n",
    "                           index=categorical_nominal_df.index)\n",
    "display(categorical_nominal_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e04b45c",
   "metadata": {},
   "source": [
    "# Lab Comparing Regression Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce208e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77f9a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define X and y\n",
    "X = concatenated_df.drop(['total_claim_amount'],axis=1)\n",
    "y = concatenated_df['total_claim_amount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90359626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import sklearn train_test_split and separate the data. Set test_size=0.30 and random_state=31\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169837de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate X_train and X_test into numerical and categorical (X_train_cat , X_train_num , X_test_cat , X_test_num)\n",
    "X_train_num, X_train_cat = split_df(X_train)\n",
    "X_test_num, X_test_cat = split_df(X_test)\n",
    "#display(X_train_num)\n",
    "#display(X_train_cat)\n",
    "#display(X_test_num)\n",
    "#display(X_test_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0b5a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the categorical variables X_train_cat and X_test_cat using the OneHotEncoder setup in the previous lab. \n",
    "# Remember to use .toarray() after .transform() to endup with a numpy array. Next, cast the resulting numpy arrays into pandas DataFrames. \n",
    "# Make sure that the column names of the new dataframes are correctly setup using encoder.get_feature_names_out() and the same indexes of X_train_cat and X_test_cat\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder(handle_unknown='error',drop='first')\n",
    "encoder.fit(X_train_cat)\n",
    "X_train_cat_encoded_arr = encoder.transform(X_train_cat).toarray()\n",
    "X_test_cat_encoded_arr  = encoder.transform(X_test_cat).toarray()\n",
    "\n",
    "X_train_cat_encoded_df = pd.DataFrame(X_train_cat_encoded_arr, columns=encoder.get_feature_names_out(), index=X_train_cat.index)\n",
    "X_test_cat_encoded_df = pd.DataFrame(X_test_cat_encoded_arr, columns=encoder.get_feature_names_out(),  index=X_test_cat.index)\n",
    "#display(X_train_cat_encoded.head())\n",
    "#display(X_test_cat_encoded.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8b24af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use X_train_num to fit a power transformer. Transform BOTH X_train_num and X_test_num. \n",
    "# Next, cast the resulting numpy arrays as pandas dataframes. Make sure to set the correct columns names and to use the same indexes of X_train_num and X_test_num. \n",
    "# Name the final resulting dataframes as: X_train_num_transformed_df and X_test_num_transformed_df\n",
    "\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "power_transformer = PowerTransformer()\n",
    "power_transformer.fit(X_train_num)\n",
    "\n",
    "X_train_num_transformed_arr = power_transformer.transform(X_train_num)\n",
    "X_test_num_transformed_arr = power_transformer.transform(X_test_num)\n",
    "\n",
    "X_train_num_transformed_df = pd.DataFrame(X_train_num_transformed_arr,columns=X_train_num.columns, index=X_train_num.index )\n",
    "X_test_num_transformed_df = pd.DataFrame(X_test_num_transformed_arr, columns=X_test_num.columns, index=X_test_num.index)\n",
    "\n",
    "#display(X_train_num_transformed_df)\n",
    "#display(X_test_num_transformed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6407c963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat X_train_num_transformed_df and X_train_cat_encoded_df into X_train_new \n",
    "# and X_test_num_transformed_df and X_test_cat_encoded_df into X_test_new\n",
    "\n",
    "X_train_new = pd.concat([X_train_num_transformed_df, X_train_cat_encoded_df], axis =1)\n",
    "X_test_new = pd.concat([X_test_num_transformed_df, X_test_cat_encoded_df], axis =1)\n",
    "#display(X_train_new.head())\n",
    "#display(X_train_new.shape)\n",
    "#display(X_test_new.head())\n",
    "#display(X_test_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572a94a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a MinMax scaler using X_train_new and transform X_train_new and X_test_new. \n",
    "# Create new pandas dataframes from the resulting numpy arrays. Remember to set the correct columns names and indexes. \n",
    "# Name the resulting dataframes as: X_train_new_scaled_df and X_test_new_scaled_df\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler() \n",
    "scaler.fit(X_train_new)\n",
    "X_train_new_scaled_arr = scaler.transform(X_train_new)\n",
    "X_test_new_scaled_arr = scaler.transform(X_test_new)\n",
    "\n",
    "X_train_new_scaled_df = pd.DataFrame(X_train_new_scaled_arr, columns = X_train_new.columns, index=X_train_new.index)\n",
    "X_test_new_scaled_df  = pd.DataFrame(X_test_new_scaled_arr,  columns = X_test_new.columns, index=X_test_new.index  )\n",
    "\n",
    "#display(X_train_new_scaled_df)\n",
    "#display(X_test_new_scaled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9606d0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a simple linear regression model using X_train_new_scaled_df, and get the predictions for the train and test sets\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lm = LinearRegression()\n",
    "lm.fit(X_train_new_scaled_df,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b356b456",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = lm.predict(X_train_new_scaled_df)\n",
    "y_pred_test = lm.predict(X_test_new_scaled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a984cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a function that given a model prediction and real values returns a pandas dataframe with the following table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3d44b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_error_metrics (y_true, y_pred) -> pd.DataFrame:\n",
    "    from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "    MAE = mean_absolute_error(y_true, y_pred)\n",
    "    MSE = mean_squared_error(y_true, y_pred)\n",
    "    RMSE = mean_squared_error(y_true, y_pred, squared=False)\n",
    "    MAPE = mean_absolute_percentage_error(y_true, y_pred)\n",
    "    R2= r2_score(y_true, y_pred)\n",
    "    \n",
    "    results = {\"Error_metric\": ['MAE', 'MSE', 'RMSE', 'MAPE', 'R2'],\n",
    "               \"Value\": [MAE, MSE, RMSE, MAPE, R2]}\n",
    "\n",
    "    results_df = pd.DataFrame(results).round(2)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f382aa92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the linear model predictions using the previous function on the TRAIN and TEST sets\n",
    "calculate_error_metrics(y_train, y_pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175c2655",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_error_metrics(y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bae917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now define a function that takes as an input: list of models, X_train and y_train \n",
    "# to train several model (with default values) so we can train a lot of them without repeating code. \n",
    "# The function must return the list of trained models.\n",
    "\n",
    "def model_training (model_list: list, X_train, y_train) -> list:\n",
    "    trained_models = []\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.neighbors import KNeighborsRegressor\n",
    "    from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "    for model_name in model_list:\n",
    "        valid_input = True\n",
    "        if model_name == 'LinearRegression':\n",
    "            model = LinearRegression()\n",
    "        elif model_name == 'KNeighborsRegressor':\n",
    "            model = KNeighborsRegressor()\n",
    "        elif model_name  == 'MLPRegressor':\n",
    "            model = MLPRegressor()\n",
    "        else:\n",
    "            print(\"Warning: Unrecognized model\", model_name)\n",
    "            valid_input = False\n",
    "        \n",
    "        if valid_input:\n",
    "            model.fit(X_train, y_train)\n",
    "            trained_models.append(model)\n",
    "        \n",
    "    return trained_models\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39380bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the function to train the following models (with default settings):\n",
    "# LinearRegression, KNeighborsRegressor, MLPRegressor\n",
    "\n",
    "[lm, knn, mlp]= model_training(['LinearRegression','KNeighborsRegressor', 'MLPRegressor'], X_train_new_scaled_df, y_train)\n",
    "\n",
    "y_train_lm_pred = lm.predict(X_train_new_scaled_df)\n",
    "y_train_knn_pred = knn.predict(X_train_new_scaled_df)\n",
    "y_train_mlp_pred = mlp.predict(X_train_new_scaled_df)\n",
    "\n",
    "y_test_lm_pred = lm.predict(X_test_new_scaled_df)\n",
    "y_test_knn_pred = knn.predict(X_test_new_scaled_df)\n",
    "y_test_mlp_pred = mlp.predict(X_test_new_scaled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589937e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the models with the function created earlier in the TRAIN and TEST sets. \n",
    "# Which model performs best with the default options?\n",
    "print(\"TRAIN SET:\\n\")\n",
    "print(\"Linear Model:\")\n",
    "display(calculate_error_metrics(y_train, y_train_lm_pred))\n",
    "print(\"\\nKnearest neighbours:\")\n",
    "display(calculate_error_metrics(y_train, y_train_knn_pred))\n",
    "print(\"\\nMLP Regressor:\")\n",
    "display(calculate_error_metrics(y_train, y_train_mlp_pred))\n",
    "\n",
    "print(\"\\nTEST SET:\\n\")\n",
    "print(\"Linear Model:\")\n",
    "display(calculate_error_metrics(y_test, y_test_lm_pred))\n",
    "print(\"\\nKnearest neighbours:\")\n",
    "display(calculate_error_metrics(y_test, y_test_knn_pred))\n",
    "print(\"\\nMLP Regressor:\")\n",
    "display(calculate_error_metrics(y_test, y_test_mlp_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04584221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the train set, the K Nearest neighbor is showing the best model performance according to all error metrics except the MAPE is showing that the MLP regressor is performing best.\n",
    "# In the test set, The MLP regressor is showing the best model performance according to all the error metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
